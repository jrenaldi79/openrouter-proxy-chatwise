/**
 * Integration tests for streaming proxy functionality
 *
 * These tests make actual HTTP requests to the proxy server to verify
 * streaming behavior for both new sessions (balance injection) and
 * follow-up conversations (direct streaming handler).
 *
 * IMPORTANT: These tests require:
 * 1. The proxy server to be running on localhost:3000
 * 2. OPENROUTER_TEST_API_KEY environment variable set
 */

import axios from 'axios';

const PROXY_URL = 'http://localhost:3000';
const TEST_API_KEY = process.env.OPENROUTER_TEST_API_KEY;

// Skip all tests if no API key or not in integration test mode
const shouldRunIntegrationTests =
  TEST_API_KEY && process.env.RUN_STREAMING_INTEGRATION === 'true';

describe('Streaming Proxy Integration Tests', () => {
  beforeAll(() => {
    if (!shouldRunIntegrationTests) {
      console.log(
        'Skipping streaming integration tests (set RUN_STREAMING_INTEGRATION=true and OPENROUTER_TEST_API_KEY to run)'
      );
    }
  });

  describe('New Session (Balance Injection Path)', () => {
    it('should successfully stream response for new session (2 messages)', async () => {
      if (!shouldRunIntegrationTests) {
        return;
      }

      const response = await axios({
        method: 'POST',
        url: `${PROXY_URL}/v1/chat/completions`,
        headers: {
          Authorization: `Bearer ${TEST_API_KEY}`,
          'Content-Type': 'application/json',
          'User-Agent': 'ChatWise/1.0', // Triggers ChatWise detection
        },
        data: {
          model: 'google/gemini-2.5-flash',
          messages: [
            { role: 'system', content: 'You are a helpful assistant.' },
            { role: 'user', content: 'Say "hello" and nothing else.' },
          ],
          stream: true,
          max_tokens: 50,
        },
        responseType: 'stream',
        timeout: 30000,
      });

      expect(response.status).toBe(200);

      // Collect the streamed data
      let data = '';
      await new Promise<void>((resolve, reject) => {
        response.data.on('data', (chunk: Buffer) => {
          data += chunk.toString();
        });
        response.data.on('end', () => resolve());
        response.data.on('error', reject);
      });

      // Should have received SSE data
      expect(data.length).toBeGreaterThan(0);
      expect(data).toContain('data:');
    }, 60000);
  });

  describe('Follow-up Session (Direct Streaming Handler Path)', () => {
    /**
     * This test replicates the exact failure pattern observed in production:
     * - messageCount: 4 (follow-up conversation)
     * - model: anthropic/claude-sonnet-4.5
     * - "SKIPPING: Not a new session" â†’ goes to handleStreamingRequest
     * - Results in "socket hang up" with chunkCount: 0
     */
    it('should successfully stream response for follow-up session (4 messages)', async () => {
      if (!shouldRunIntegrationTests) {
        return;
      }

      const response = await axios({
        method: 'POST',
        url: `${PROXY_URL}/v1/chat/completions`,
        headers: {
          Authorization: `Bearer ${TEST_API_KEY}`,
          'Content-Type': 'application/json',
          'User-Agent': 'ChatWise/1.0',
        },
        data: {
          model: 'google/gemini-2.5-flash', // Using Gemini for faster response
          messages: [
            { role: 'system', content: 'You are a helpful assistant.' },
            { role: 'user', content: 'What is 2+2?' },
            { role: 'assistant', content: '2+2 equals 4.' },
            { role: 'user', content: 'And what is 3+3?' },
          ],
          stream: true,
          max_tokens: 50,
        },
        responseType: 'stream',
        timeout: 30000,
      });

      expect(response.status).toBe(200);

      // Collect the streamed data
      let data = '';
      await new Promise<void>((resolve, reject) => {
        response.data.on('data', (chunk: Buffer) => {
          data += chunk.toString();
        });
        response.data.on('end', () => resolve());
        response.data.on('error', reject);
      });

      // Should have received SSE data
      expect(data.length).toBeGreaterThan(0);
      expect(data).toContain('data:');
    }, 60000);

    /**
     * This test replicates the large body size failure:
     * - bodySize: ~91KB
     * - model: anthropic/claude-sonnet-4.5
     * - messageCount: 4
     */
    it('should handle large conversation payload (simulated 91KB body)', async () => {
      if (!shouldRunIntegrationTests) {
        return;
      }

      // Create a large conversation to simulate the 91KB body
      const largeContent = 'x'.repeat(20000); // ~20KB of content per message

      const response = await axios({
        method: 'POST',
        url: `${PROXY_URL}/v1/chat/completions`,
        headers: {
          Authorization: `Bearer ${TEST_API_KEY}`,
          'Content-Type': 'application/json',
          'User-Agent': 'ChatWise/1.0',
        },
        data: {
          model: 'google/gemini-2.5-flash',
          messages: [
            { role: 'system', content: 'You are a helpful assistant.' },
            { role: 'user', content: `Here is some context: ${largeContent}` },
            {
              role: 'assistant',
              content: `I understand the context: ${largeContent.substring(0, 100)}...`,
            },
            { role: 'user', content: 'Summarize in one word.' },
          ],
          stream: true,
          max_tokens: 50,
        },
        responseType: 'stream',
        timeout: 60000,
      });

      expect(response.status).toBe(200);

      // Collect the streamed data
      let data = '';
      await new Promise<void>((resolve, reject) => {
        response.data.on('data', (chunk: Buffer) => {
          data += chunk.toString();
        });
        response.data.on('end', () => resolve());
        response.data.on('error', reject);
      });

      expect(data.length).toBeGreaterThan(0);
    }, 120000);
  });

  describe('Non-ChatWise Client (Direct Streaming Handler)', () => {
    /**
     * Test streaming for non-ChatWise clients which always go through
     * the direct streaming handler regardless of message count
     */
    it('should stream response for non-ChatWise client', async () => {
      if (!shouldRunIntegrationTests) {
        return;
      }

      const response = await axios({
        method: 'POST',
        url: `${PROXY_URL}/v1/chat/completions`,
        headers: {
          Authorization: `Bearer ${TEST_API_KEY}`,
          'Content-Type': 'application/json',
          'User-Agent': 'TestClient/1.0', // NOT ChatWise
        },
        data: {
          model: 'google/gemini-2.5-flash',
          messages: [
            { role: 'system', content: 'You are helpful.' },
            { role: 'user', content: 'Say hello.' },
          ],
          stream: true,
          max_tokens: 50,
        },
        responseType: 'stream',
        timeout: 30000,
      });

      expect(response.status).toBe(200);

      let data = '';
      await new Promise<void>((resolve, reject) => {
        response.data.on('data', (chunk: Buffer) => {
          data += chunk.toString();
        });
        response.data.on('end', () => resolve());
        response.data.on('error', reject);
      });

      expect(data.length).toBeGreaterThan(0);
      expect(data).toContain('data:');
    }, 60000);
  });
});

/**
 * Simpler unit-style test that doesn't require the server running
 * This tests the actual failure pattern by checking what goes wrong
 */
describe('Streaming Handler Failure Analysis', () => {
  it('should identify the difference between axios and https.request approaches', () => {
    // This documents the key difference:
    // - Balance injection uses axios with validateStatus: () => true
    // - handleStreamingRequest uses raw https.request

    // The axios approach:
    const axiosConfig = {
      method: 'POST',
      url: 'https://openrouter.ai/api/v1/chat/completions',
      headers: {
        Authorization: 'Bearer xxx',
        'Content-Type': 'application/json',
      },
      data: { model: 'test', messages: [], stream: true },
      responseType: 'stream' as const,
      validateStatus: (): boolean => true,
    };

    // Axios automatically handles:
    // 1. Content-Length calculation
    // 2. Proper encoding
    // 3. Connection management
    // 4. Error handling

    expect(axiosConfig.responseType).toBe('stream');
    expect(typeof axiosConfig.validateStatus).toBe('function');
  });

  it('should calculate correct body size for large payloads', () => {
    // Simulating the exact failure case: ~91KB body
    const largeContent = 'x'.repeat(20000);
    const body = {
      model: 'anthropic/claude-sonnet-4.5',
      messages: [
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'user', content: largeContent },
        { role: 'assistant', content: largeContent },
        { role: 'user', content: 'Continue.' },
      ],
      stream: true,
    };

    const bodyString = JSON.stringify(body);
    const bodySize = Buffer.byteLength(bodyString, 'utf8');

    // This should be similar to the 91KB observed in production
    expect(bodySize).toBeGreaterThan(40000);

    // Verify Content-Length would be set correctly
    expect(Number.isInteger(bodySize)).toBe(true);
  });
});
